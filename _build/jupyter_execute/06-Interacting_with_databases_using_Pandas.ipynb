{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ruled-premium",
   "metadata": {},
   "source": [
    "# Interacting with databases using Pandas\n",
    "\n",
    "In this tutorial, we show how to use Pandas data frames to interact with SQL-based and graph-based databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-training",
   "metadata": {},
   "source": [
    "## Data available in different sources\n",
    "\n",
    "Often, when you have to deal with and reuse existing data, the answer to a query can be possible only by combining data available in different databases. In addition, such databases can expose their data using different technologies (e.g. an SQLite database and an RDF triplestore). Thus, it is important to have a smooth method that allows one to take data from different sources, to expose these data according to a similar interface, and finally to make some additional operation on these data that, in principle, can be seen as coming from a unique abstract source.\n",
    "\n",
    "Pandas, thanks to its standard library and additional plugins developed for it, enables us to use it as a proxy model for getting and comparing data coming from different sources (and even different formats). A few tutorials ago, indeed, we have seen how to read data stored as CSV documents using Pandas. We can use similar functions to read a result of a query sent to a database as it is a source of information. In this tutorial, we see how to do it with SQLite and Blazegraph, i.e. the two databases used in the previous tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-lease",
   "metadata": {},
   "source": [
    "## Reading data from SQLite\n",
    "\n",
    "Pandas makes available the [method `read_sql`](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) which enables us, among the other things, to query an SQL-based database using an SQL query and to expose the answer returned as a classic Pandas data frame. This function takes in input two mandatory parameters that are the SQL query to execute on the database and the connection to it, and returns a data frame built on the data and the parameter specified in the SQL query. For instance, the following code takes the title of all the journal articles included in the table `JournalArticle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "steady-beginning",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlite3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connect\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_sql\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m connect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../04/publications.db\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[1;32m      5\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT title FROM JournalArticle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from sqlite3 import connect\n",
    "from pandas import read_sql\n",
    "\n",
    "with connect(\"../04/publications.db\") as con:\n",
    "    query = \"SELECT title FROM JournalArticle\"\n",
    "    df_sql = read_sql(query, con)\n",
    "    \n",
    "df_sql  # show the content of the result of the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-ranch",
   "metadata": {},
   "source": [
    "It is worth mentioning that, to enable the correct definition of the results of the query into a data frame, it is always better first to create all the necessary data frames within the `with` clause, and then start to work on them \"offline\", once the connection to the database has been closed. Otherwise, you could observe some unexpected behaviours.\n",
    "\n",
    "Finally, it is worth mentioning that the data type used in the database are converted into the appropriate data type in Pandas. Thus, if a column has been defined as containing integers in the database, we get back the same data type for the column in the data frame. This is clear when we try to retrieve, for instance, an entire table from the SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "focused-sailing",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../04/publications.db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[1;32m      2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM JournalArticle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     df_journal_article_sql \u001b[38;5;241m=\u001b[39m read_sql(query, con)\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "with connect(\"../04/publications.db\") as con:\n",
    "    query = \"SELECT * FROM JournalArticle\"\n",
    "    df_journal_article_sql = read_sql(query, con)\n",
    "\n",
    "# Show the series of the column 'publicationYear', which as 'dtype'\n",
    "# specifies 'int64', as expected\n",
    "df_journal_article_sql[\"publicationYear\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-finder",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading data from Blazegraph\n",
    "\n",
    "Even if Pandas does not make available any reading method to interact with RDF triplestores, some developers has implemented a facility that permits us to interact directly with a SPARQL endpoint provided by an RDF triplestore such as Blazegraph, i.e. the [library `sparql_dataframe`](https://github.com/lawlesst/sparql-dataframe). This library is a wrapper for a SPARQL query and shows the answer to such a query as a Pandas data frame. We can install the library using the usual command:\n",
    "\n",
    "```\n",
    "pip install sparql_dataframe\n",
    "```\n",
    "\n",
    "The function `get` is called to perform such an operation, and it takes in input three parameters: the URL of the SPARQL endpoint to contact, the query to execute, and a boolean specifying if to contact the SPARQL endpoint using the [POST HTTP method](https://en.wikipedia.org/wiki/POST_(HTTP)) (strongly suggested, otherwise it could not work correctly). An example of execution of such a function is shown in the following excerpt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colored-rings",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparql_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparql_dataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m      3\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:9999/blazegraph/sparql\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mPREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mPREFIX schema: <https://schema.org/>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparql_dataframe'"
     ]
    }
   ],
   "source": [
    "from sparql_dataframe import get\n",
    "\n",
    "endpoint = \"http://127.0.0.1:9999/blazegraph/sparql\"\n",
    "query = \"\"\"\n",
    "PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX schema: <https://schema.org/>\n",
    "\n",
    "SELECT ?journal_article ?title\n",
    "WHERE {\n",
    "    ?journal_article rdf:type schema:ScholarlyArticle .\n",
    "    ?journal_article schema:name ?title .\n",
    "}\n",
    "\"\"\"\n",
    "df_sparql = get(endpoint, query, True)\n",
    "df_sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f2e46-a3b7-425e-8fcb-d0e25369f7ce",
   "metadata": {},
   "source": [
    "Due to the implementation of the `get` function in the `sparql_dataframe` package, though, the values returned by running the SPARQL query will be inferred automatically by looking at all the values of a certain column. Thus, if one wants to change the data type of the values associated to a particular column, one has to cast the column on purpose and the reassigning the column to the data frame. For instance, let us build a query that takes information of all the publications available in the triplestore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a57bdc-d1fe-4878-a3f2-e9a46f223a69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m publication_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mPREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mPREFIX schema: <https://schema.org/>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m df_publications_sparql \u001b[38;5;241m=\u001b[39m \u001b[43mget\u001b[49m(endpoint, publication_query, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m df_publications_sparql\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get' is not defined"
     ]
    }
   ],
   "source": [
    "publication_query = \"\"\"\n",
    "PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX schema: <https://schema.org/>\n",
    "\n",
    "SELECT ?internalId ?doi ?publicationYear ?title ?issue ?volume ?publicationVenue\n",
    "WHERE {\n",
    "    VALUES ?type {\n",
    "        schema:ScholarlyArticle\n",
    "        schema:Chapter\n",
    "    }\n",
    "    \n",
    "    ?internalId rdf:type ?type .\n",
    "    ?internalId schema:identifier ?doi .\n",
    "    ?internalId schema:datePublished ?publicationYear .\n",
    "    ?internalId schema:name ?title .\n",
    "    ?internalId schema:isPartOf ?publicationVenue .\n",
    "        \n",
    "    OPTIONAL {\n",
    "        ?internalId schema:issueNumber ?issue .\n",
    "        ?internalId schema:volumeNumber ?volume .\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "df_publications_sparql = get(endpoint, publication_query, True)\n",
    "df_publications_sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724cf7f4-38b0-4b75-8a0f-16c5f9e95bac",
   "metadata": {},
   "source": [
    "It is worth mentioning that the optional group in the SPARQL query (`OPTIONAL { ... }`) is used to allow information to be added to the solution if it is available, otherwise the related variables will be left empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a633f2-645a-4250-ae6a-df4b47684475",
   "metadata": {},
   "source": [
    "## Fixing some issues\n",
    "\n",
    "As you can observed from the result of the previous query, the data frame created contains some basic information depicted by the variable names chosen, that are specified in the query itself for being equal to those returned in the last SQL query done above. \n",
    "\n",
    "However, one unexpected behaviour is the way the columns `issue` and `volume` is handled. To see this, we use the [attribute `dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html) of our data frame to see how things are handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad5aa98-74c5-4553-bd74-6ecef86e80a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_publications_sparql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_publications_sparql\u001b[49m\u001b[38;5;241m.\u001b[39mdtypes\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_publications_sparql' is not defined"
     ]
    }
   ],
   "source": [
    "df_publications_sparql.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdf934-3a22-4e9d-a606-d578bcafa294",
   "metadata": {},
   "source": [
    "As you can see, the two columns mentioned above have been assigned with a float data type, which has been inferred by Pandas by looking at the values of these two columns. In order to change it into an appropriate kind of value, e.g. a string, we have to overwrite the data type of the entire data frame (using the [method `astype`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) that takes in input the new data type) and/or the data type of specific columns. For doing the last operation, we have to reassign the columns with the new types to the data frame using the following syntax:\n",
    "\n",
    "```\n",
    "<data frame>[<column name>] = <data frame>[<column name>].astype(<new data type>)\n",
    "```\n",
    "\n",
    "For instance, to reassign the columns `issue` and `volume` to the type `\"string\"`, we can run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb5b55e-8e3e-49ea-9659-9b10d6957239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_publications_sparql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_publications_sparql[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_publications_sparql\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_publications_sparql[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_publications_sparql[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m df_publications_sparql\u001b[38;5;241m.\u001b[39mdtypes\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_publications_sparql' is not defined"
     ]
    }
   ],
   "source": [
    "df_publications_sparql[\"issue\"] = df_publications_sparql[\"issue\"].astype(\"string\")\n",
    "df_publications_sparql[\"volume\"] = df_publications_sparql[\"volume\"].astype(\"string\")\n",
    "\n",
    "df_publications_sparql.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435b381-8303-4ed2-b41d-7e3321617a3b",
   "metadata": {},
   "source": [
    "Similarly, if you want to replace the `NaN` values associated to the same two columns when no value is available, you can use the data frame [method `fillna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html), which enables one to replace all `NaN` in the data frame with a value of your choice passed as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1c7490-5425-46d7-86a6-77fa707ebe57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_publications_sparql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_publications_sparql \u001b[38;5;241m=\u001b[39m \u001b[43mdf_publications_sparql\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_publications_sparql\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_publications_sparql' is not defined"
     ]
    }
   ],
   "source": [
    "df_publications_sparql = df_publications_sparql.fillna(\"\")\n",
    "\n",
    "df_publications_sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f7dc9-b0b3-47c8-9969-be81b79df97c",
   "metadata": {},
   "source": [
    "Of course, this allowed us to remove all `NaN` values. However, if you look at the table and in particular to the columns `issue` and `volume`, you can see something that is still a bit in these two columns. \n",
    "\n",
    "Indeed, the two strings defining issues and volumes associated with an article are, actually, the mere cast of the floating value into a string and, as such, they contain the `.0` part of the float that we need to remove. Since the same pattern is repeated in all the values of these two columns, we could apply a similar operation to all their values to clean them up. For doing that, we use the [method `apply`](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) of the class `Series`, which allows us to apply an input function to all the values of a column and to store, in each value, what such a function returns.\n",
    "\n",
    "A function that would allow us perform such an operation is the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2866072-8522-4fd3-abd6-d3d4c0e96452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_dotzero(s):\n",
    "    return s.replace(\".0\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae741f-ff5f-44b2-b362-1094da1c070a",
   "metadata": {},
   "source": [
    "The function above takes in input a string (i.e. the value of a cell) and remove the string `\".0\"` from there, if present. Thus, passing this function to the method `apply` of each column and then to assign the modified column back to the data frame will fix the issue, as shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d785d5-ffb8-4a97-8e9b-d7a51f01e5bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_publications_sparql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_publications_sparql[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_publications_sparql\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_dotzero)\n\u001b[1;32m      2\u001b[0m df_publications_sparql[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_publications_sparql[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_dotzero)\n\u001b[1;32m      4\u001b[0m df_publications_sparql\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_publications_sparql' is not defined"
     ]
    }
   ],
   "source": [
    "df_publications_sparql[\"issue\"] = df_publications_sparql[\"issue\"].apply(remove_dotzero)\n",
    "df_publications_sparql[\"volume\"] = df_publications_sparql[\"volume\"].apply(remove_dotzero)\n",
    "\n",
    "df_publications_sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0f155-da4c-46f0-b37c-14933a96287a",
   "metadata": {},
   "source": [
    "## Combining data\n",
    "\n",
    "In the previous section, we have introced how to obtain data from existing databases and how to manipulate them using Pandas. However, in real case scenarios, an answer to a certain query can arrive only from mixing partial data from two distinct databases. Thus, it is important to implement some mechanisms to mash data up together, clean them if needed (e.g. removing duplicates), and to return them in a certain order (e.g. alphabetically). Of course, Pandas can be used to perform all these operations.\n",
    "\n",
    "Suppose that we want to find, by querying all the databases, all the titles and year of publication of all publications they contain (independently from their type), ordered from the oldest one to the newest one. To simplify the job for this tutorial, we could consider the two data frames computed before, i.e. `df_journal_article_sql` and `df_publications_sparql`, as the two coming from two different databases.\n",
    "\n",
    "First of all, we need something that allows us to concat two or more data frames together. However, in order to do that, it is important that, first of all, all the data frames to contact share the same columns. Thus, if necessary, it is important to rename the columns as we have seen in a previous tutorial. In this case, instead, we have already created the data frames with the same column names and, as such, we can proceed with the concat operation, i.e. obtaining a new data frame by concatenating the rows contained in both the data frames.\n",
    "\n",
    "This operation is implemented by the [function `concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html), that takes in input a list of data frames and return a new data frame with all the rows concatenated. In addition, it can also take in input the named parameter `ignore_index` that, if set to `True`, will reindex all the rows from the beginning in the new data frame, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c99f9a-40d4-47a3-8117-6fe29a121997",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat\n\u001b[1;32m      3\u001b[0m df_union \u001b[38;5;241m=\u001b[39m concat([df_journal_article_sql, df_publications_sparql], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m df_union\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from pandas import concat\n",
    "\n",
    "df_union = concat([df_journal_article_sql, df_publications_sparql], ignore_index=True)\n",
    "df_union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f0100-d150-4dc5-9d14-d0aeea943131",
   "metadata": {},
   "source": [
    "After having obtained a new data frame concatenating the other two, we need to filter out duplicates. Once can follow different approaches for doing so. In this context, we will use the DOIs of the publications to perform the filtering. \n",
    "\n",
    "A [DOI (Digital Object Identifier)](https://en.wikipedia.org/wiki/Digital_object_identifier) is a persistent identifier used to identify publications uniquely worldwide. Thus, if a publication is included in two distinct databases, it should have the same DOI despite the local identifiers the databases may use.\n",
    "\n",
    "Once this aspect is clear, we can perform a removal of rows using the [method `drop_duplicates`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) of the class `DataFrame`. This method allows one to specify the optional named parameter `subset` with the list of columns names to use to identify similar rows. If such a named parameter is not specified, only identical rows (those having all the values in full match) are removed from data frame. Thus, we can perform the removal of duplicates as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "699a8aca-a807-4f02-8b7d-3ed251fb3ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_union_no_duplicates \u001b[38;5;241m=\u001b[39m \u001b[43mdf_union\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m df_union_no_duplicates\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_union' is not defined"
     ]
    }
   ],
   "source": [
    "df_union_no_duplicates = df_union.drop_duplicates(subset=[\"doi\"])\n",
    "df_union_no_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd10775-4660-483b-ab60-5a5e84eb476b",
   "metadata": {},
   "source": [
    "Then, we have finally to sort rows in ascending order considering the publication year, and then to return just the columns publication year and title and year of publication of each row. In Pandas, the sorting can be performed using the [method `sort_values`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) of the class `DataFrame`, that takes in input the name of the column to use to perform the sorting, as shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25b8d455-4a93-4966-abfc-94d70501e3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_union_no_duplicates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_union_no_duplicates_sorted \u001b[38;5;241m=\u001b[39m \u001b[43mdf_union_no_duplicates\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublicationYear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_union_no_duplicates_sorted\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_union_no_duplicates' is not defined"
     ]
    }
   ],
   "source": [
    "df_union_no_duplicates_sorted = df_union_no_duplicates.sort_values(\"publicationYear\")\n",
    "df_union_no_duplicates_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d244dc-9d78-44bd-89f2-f913d5388c2c",
   "metadata": {},
   "source": [
    "Finally, to select a sub-data frame, we use the approach adopted in past tutorial, by creating a new data frame selecting only some of the columns of another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ee9442-c74c-4c3e-8700-84181c53dddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_union_no_duplicates_sorted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_final \u001b[38;5;241m=\u001b[39m \u001b[43mdf_union_no_duplicates_sorted\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublicationYear\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      2\u001b[0m df_final\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_union_no_duplicates_sorted' is not defined"
     ]
    }
   ],
   "source": [
    "df_final = df_union_no_duplicates_sorted[[\"title\", \"publicationYear\"]]\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02dbcc3-e4f4-428b-a516-4da7de80f0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}